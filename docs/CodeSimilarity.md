# csim

This project analyzes structural similarity between Python source code using
ANTLR4-generated ASTs and Zhang–Shasha Tree Edit Distance (ZSS).

For next versions, support for additional programming languages will be added.

## AST Normalization and ZSS Tree Construction

This project compares Python source code by measuring **structural similarity**
between normalized Abstract Syntax Trees (ASTs).
The ASTs are generated using **ANTLR4** and compared using
**Zhang–Shasha Tree Edit Distance (ZSS)**.

To ensure **experimental reproducibility, efficiency, and traceability**,
the following design decisions are applied.

### Rule and Token Exclusion Strategy

Instead of relying on rule names or regular expressions, this project
exclusively uses:

- `ruleIndex` for parser rules
- `token.type` for lexer tokens

Both identifiers are **numeric constants generated by ANTLR**, which guarantees:

- O(1) comparisons
- Independence from rule names
- Robustness across grammar or version changes

### Collapsed Parser Rules
Collapsed rules are defined using explicit sets of rule indices and token types.

```python
COLLAPSED_RULE_INDICES: set[int]
```

These rules are collapsed to reduce AST depth and noise.
When a collapsed rule is visited, its children are promoted upward in the tree, effectively removing the collapsed rule from the AST structure.

#### Excluded Parser Rules and Lexer Tokens

Excluded rules are defined as:

```python
EXCLUDED_RULE_INDICES: set[int]
```

Rules are excluded when they do **not contribute meaningful semantic or
structural information**.

Excluded tokens are defined as:

```python
EXCLUDED_TOKEN_TYPES: set[int]
```

Tokens are excluded when they do **not contribute meaningful semantic or
structural information**. 


### ZSS Tree Construction

The normalized tree is constructed using an **ANTLR Visitor**.

Each node in the resulting ZSS tree represents one of:

- A **parser rule** (`ruleIndex`)
- A **relevant lexer token** (`token.type`)

All nodes are labeled using **integer identifiers only**.

For preventing use the same integer for both rule and token nodes, token types are offset by a constant value.

```python
TOKEN_TYPE_OFFSET = 1000
```

### Hash-Based Pruning
To optimize tree size while preserving essential structure, a hash-based pruning strategy is employed.
The pruning process involves:
1. **Hashing Subtrees:** Each subtree is hashed to create a unique identifier.
2. **Identifying Redundant Subtrees:** Subtrees with identical hashes are identified as redundant.
3. **Pruning Redundant Subtrees:** Redundant subtrees are pruned from the AST, retaining only one instance of each unique subtree.

Excluded rules are considered for prunning are defined as:

```python
HASHED_RULE_INDICES: set[int]
```

### Tree Edit Distance Configuration

Similarity is computed using **Zhang–Shasha Tree Edit Distance (ZSS)**.

Recommended base costs:

| Operation | Cost |
|---------|------|
| Insert  | 1 |
| Delete | 1 |
| Update (same label) | 0 |
| Update (different label) | 1 |

### Similarity Calculation
Similarity is calculated as:

```python
def SimilarityIndex(d, T1, T2):
    m = max(T1, T2)
    s = 1 - (d / m)
    s = round(s, 2)
    return s
```

Where:
- `d`: Tree edit distance between the two ASTs
- `T1`: Number of nodes in the first AST
- `T2`: Number of nodes in the second AST
The similarity index ranges from `0.0` (completely different) to `1.0` (identical).


### Experimental Traceability

These design choices provide:

- Deterministic and reproducible comparisons
- Clear justification of AST transformations
- Fine-grained control over abstraction levels
